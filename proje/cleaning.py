"""
import pandas as pd           # CSV dosyasÄ±nÄ± okumak ve iÅŸlemek iÃ§in
import re                     # Metin temizliÄŸi (regex ile)
import string                 # Noktalama iÅŸaretlerini tanÄ±mak iÃ§in
import nltk                   # DoÄŸal dil iÅŸleme araÃ§larÄ±

# NLTK araÃ§larÄ±nÄ± iÃ§e aktar
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Gerekli NLTK veri setlerini indir
nltk.download('punkt')        # kelime ayÄ±rÄ±cÄ±
nltk.download('stopwords')    # gereksiz kelimeler listesi
nltk.download('wordnet')      # kÃ¶k bulucu sÃ¶zlÃ¼k
nltk.download('omw-1.4')      # kÃ¶k bulma iÃ§in ekstra veri

# CSV dosyasÄ±nÄ± oku
df = pd.read_csv("cryptonews.csv")

# Biz haber baÅŸlÄ±ÄŸÄ± Ã¼zerinde Ã§alÄ±ÅŸacaÄŸÄ±z
df['raw_text'] = df['title']

# Temizleme fonksiyonu
def preprocess(text):
    text = text.lower()  # KÃ¼Ã§Ã¼k harf yap
    text = re.sub(r'\d+', '', text)  # SayÄ±larÄ± kaldÄ±r
    text = text.translate(str.maketrans('', '', string.punctuation))  # Noktalama kaldÄ±r
    tokens = word_tokenize(text)  # Kelimelere ayÄ±r
    tokens = [w for w in tokens if w not in stopwords.words('english')]  # Stopword temizle
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(w) for w in tokens]  # KÃ¶k bul
    return " ".join(tokens)

# Temizleme iÅŸlemini tÃ¼m haberlere uygula
df['clean_text'] = df['raw_text'].apply(preprocess)

# SonuÃ§larÄ± gÃ¶ster
print(df[['raw_text', 'clean_text']].head())

# âœ… TemizlenmiÅŸ veriyi kaydet (etiketleme aÅŸamasÄ±nda kullanÄ±lacak)
df.to_csv("cleaned_news.csv", index=False)
print("âœ… TemizlenmiÅŸ veri 'cleaned_news.csv' dosyasÄ±na kaydedildi.")
"""
import pandas as pd
import re
import string
import nltk

# NLTK araÃ§larÄ±nÄ± iÃ§e aktar
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Gerekli NLTK veri setlerini indir (ilk Ã§alÄ±ÅŸtÄ±rmada)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# ğŸ“Œ 1. CSV dosyasÄ±nÄ± oku
df = pd.read_csv("cryptonews.csv")

# ğŸ“Œ 2. BaÅŸlÄ±k sÃ¼tununu kontrol et, boÅŸlarÄ± sil
df = df.dropna(subset=['title'])
df['raw_text'] = df['title']

# ğŸ“Œ 3. Temizleme fonksiyonu
def preprocess(text):
    text = str(text).lower()  # KÃ¼Ã§Ã¼k harfe Ã§evir
    text = re.sub(r'\d+', '', text)  # SayÄ±larÄ± kaldÄ±r
    text = text.translate(str.maketrans('', '', string.punctuation))  # Noktalama kaldÄ±r
    tokens = word_tokenize(text)  # Kelimelere ayÄ±r
    tokens = [w for w in tokens if w not in stopwords.words('english')]  # Stopword temizle
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(w) for w in tokens]  # KÃ¶k bul
    return " ".join(tokens)

# ğŸ“Œ 4. Temizleme iÅŸlemini tÃ¼m metinlere uygula
df['clean_text'] = df['raw_text'].apply(preprocess)

# ğŸ“Œ 5. Temizleme sonrasÄ± oluÅŸan boÅŸ metinleri Ã§Ä±kar
df = df.dropna(subset=['clean_text'])

# ğŸ“Œ 6. SonuÃ§larÄ± gÃ¶ster
print("\nTemizlenmiÅŸ Ã¶rnek veri:")
print(df[['raw_text', 'clean_text']].head())

# ğŸ“Œ 7. Yeni dosya olarak kaydet
df.to_csv("cleaned_news.csv", index=False)
print("\nâœ… TemizlenmiÅŸ veri 'cleaned_news.csv' dosyasÄ±na kaydedildi.")
