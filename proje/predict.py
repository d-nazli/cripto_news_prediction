"""
import pickle
import re
import string
import nltk

import pandas as pd
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from save_result import save_result, detect_coin

# Gerekli NLTK verilerini indir (bir kereye mahsus)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# ğŸ“¦ KayÄ±tlÄ± model ve vektÃ¶rleÅŸtiriciyi yÃ¼kle
with open("trained_model.pkl", "rb") as f:
    model = pickle.load(f)

with open("vectorizer.pkl", "rb") as f:
    vectorizer = pickle.load(f)

# ğŸ”§ Temizleme fonksiyonu (model eÄŸitimiyle aynÄ± olmalÄ±!)
def preprocess(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    tokens = word_tokenize(text)
    tokens = [w for w in tokens if w not in stopwords.words('english')]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(w) for w in tokens]
    return " ".join(tokens)

# ğŸ§ª KullanÄ±cÄ±dan haber baÅŸlÄ±ÄŸÄ± al
user_input = input("\nğŸ“ LÃ¼tfen haber baÅŸlÄ±ÄŸÄ±nÄ± girin: ")

# Temizle ve TF-IDF vektÃ¶rÃ¼ne Ã§evir
clean_text = preprocess(user_input)
vectorized = vectorizer.transform([clean_text])

prediction = model.predict(vectorized)[0]
probability = model.predict_proba(vectorized)[0]
confidence = round(probability[1] * 100, 2)  # Etiket 1 (olumlu/gÃ¼venilir) iÃ§in yÃ¼zde

# âœ… SonuÃ§ yazdÄ±r
print("\nğŸ“Œ Girilen BaÅŸlÄ±k:", user_input)
print(f"ğŸ“Š Model gÃ¼ven skoru: %{confidence}")
if prediction == 1:
    print("âœ… Bu haber OLUMLU / GÃœVENÄ°LÄ°R olabilir.")
else:
    print("âŒ Bu haber OLUMSUZ / YANILTICI olabilir.")
"""
"""
import pickle
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

from save_result import save_result, detect_coin  # JSON kaydÄ± iÃ§in

# Gerekli NLTK verilerini indir (bir kereye mahsus)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# ğŸ“¦ KayÄ±tlÄ± model ve vektÃ¶rleÅŸtiriciyi yÃ¼kle
with open("trained_model.pkl", "rb") as f:
    model = pickle.load(f)

with open("vectorizer.pkl", "rb") as f:
    vectorizer = pickle.load(f)

# ğŸ”§ Temizleme fonksiyonu (model eÄŸitimiyle aynÄ± olmalÄ±!)
def preprocess(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    tokens = word_tokenize(text)
    tokens = [w for w in tokens if w not in stopwords.words('english')]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(w) for w in tokens]
    return " ".join(tokens)

# ğŸ§ª KullanÄ±cÄ±dan haber baÅŸlÄ±ÄŸÄ± al
user_input = input("\nğŸ“ LÃ¼tfen haber baÅŸlÄ±ÄŸÄ±nÄ± girin: ")

# ğŸ” Temizle ve vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼r
clean_text = preprocess(user_input)
vectorized = vectorizer.transform([clean_text])

# ğŸ¤– Tahmin ve doÄŸruluk oranÄ±
prediction = model.predict(vectorized)[0]
probability = model.predict_proba(vectorized)[0]
confidence = round(probability[1] * 100, 2)  # Etiket 1 (olumlu/gÃ¼venilir) iÃ§in yÃ¼zde

# ğŸ§  Tahmine gÃ¶re sonuÃ§ etiketi
prediction_label = "YÃœKSELECEK" if prediction == 1 else "DÃœÅECEK"
affected_coin = detect_coin(user_input)

# âœ… Ekrana yazdÄ±r
print("\nğŸ“Œ Girilen BaÅŸlÄ±k:", user_input)
print(f"ğŸ“Š Model gÃ¼ven skoru: %{confidence}")
if prediction == 1:
    print("âœ… Bu haber OLUMLU / GÃœVENÄ°LÄ°R olabilir.")
else:
    print("âŒ Bu haber OLUMSUZ / YANILTICI olabilir.")

# ğŸ“ JSON dosyasÄ±na sonucu kaydet
save_result(user_input, confidence, affected_coin, prediction_label)
"""


import pickle
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

from save_result import save_result, detect_coin  # JSON kaydÄ± iÃ§in

# Gerekli NLTK verilerini indir (ilk Ã§alÄ±ÅŸtÄ±rmada gerekebilir)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# ğŸ“¦ EÄŸitilmiÅŸ model ve TF-IDF vektÃ¶rleÅŸtiriciyi yÃ¼kle
with open("trained_model.pkl", "rb") as f:
    model = pickle.load(f)

with open("vectorizer.pkl", "rb") as f:
    vectorizer = pickle.load(f)

# ğŸ”§ Haber baÅŸlÄ±ÄŸÄ±nÄ± temizleyen fonksiyon
def preprocess(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    tokens = word_tokenize(text)
    tokens = [w for w in tokens if w not in stopwords.words('english')]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(w) for w in tokens]
    return " ".join(tokens)

# ğŸ§ª KullanÄ±cÄ±dan haber baÅŸlÄ±ÄŸÄ± al
user_input = input("\nğŸ“ LÃ¼tfen haber baÅŸlÄ±ÄŸÄ±nÄ± girin: ")

# ğŸ§¼ Temizle â†’ VektÃ¶rleÅŸtir â†’ Tahmin et
clean_text = preprocess(user_input)
vectorized = vectorizer.transform([clean_text])
prediction = model.predict(vectorized)[0]
probability = model.predict_proba(vectorized)[0]
confidence_score = round(probability[1] * 100, 2)  # sÄ±nÄ±f 1 (gÃ¼venilir) iÃ§in oran

# ğŸ”„ up / down etiketini belirle
prediction_label = "up" if prediction == 1 else "down"

# ğŸª™ Coin sembolÃ¼nÃ¼ belirle
affected_coin = detect_coin(user_input)

# ğŸ“¢ Sonucu yazdÄ±r
print("\nğŸ“Œ BaÅŸlÄ±k:", user_input)
print(f"ğŸ“Š GÃ¼ven skoru: %{confidence_score}")
print(f"ğŸ“ˆ Tahmin: {prediction_label}")
print(f"ğŸª™ Coin: {affected_coin}")

# ğŸ’¾ JSON dosyasÄ±na sonucu kaydet
save_result(user_input, confidence_score, affected_coin, prediction_label)
